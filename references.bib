@inproceedings{vaswani2017attention,
  title={Attention is all you need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  year={2017},
  url={https://arxiv.org/pdf/1706.03762}
},
@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020},
  url={https://dl.acm.org/doi/abs/10.5555/3495724.3496517}
},
@misc{finardi2024chronicles,
  title={The Chronicles of RAG: The Retriever, the Chunk and the Generator},
  author={Paulo Finardi and Leonardo Avila and Rodrigo Castaldoni and Pedro Gengo and Celio Larcher and Marcos Piau and Pablo Costa and Vinicius Caridá},
  year={2024},
  note={Preprint},
  url={https://arxiv.org/abs/2401.07883},
  howpublished={arXiv preprint arXiv:2401.07883},
},
@article{chen2023prompt,
  author = {Banghao Chen and Zhaofeng Zhang and Nicolas Langren{\'e} and Shengxin Zhu},
  title = {Unleashing the potential of prompt engineering in Large Language Models: a comprehensive review},
  journal = {Guangdong Provincial Key Laboratory of Interdisciplinary Research and Application for Data Science},
  year = {2023},
  url= {https://arxiv.org/pdf/2310.14735},
  note = {BNU-HKBU United International College, Beijing Normal University},
},
@misc{nvidia2024facts,
  author = {Rama Akkiraju and Anbang Xu and Deepak Bora and Tan Yu and Lu An and Vishal Seth and Aaditya Shukla and Pritam Gundecha and Hridhay Mehta and Ashwin Jha and Prithvi Raj and Abhinav Balasubramanian and Murali Maram and Guru Muthusamy and Shivakesh Reddy Annepally and Sidney Knowles and Min Du and Nick Burnett and Sean Javiya and Ashok Marannan and Mamta Kumari and Surbhi Jha and Ethan Dereszenski and Anupam Chakraborty and Subhash Ranjan and Amina Terfai and Anoop Surya and Tracey Mercer and Vinodh Kumar Thanigachalam and Tamar Bar and Sanjana Krishnan and Jasmine Jaksic and Nave Algarici and Jacob Liberman and Joey Conway and Sonu Nayyar and Justin Boitano},
  title = {Facts About Building Retrieval Augmented Generation-based Chatbots},
  institution = {NVIDIA},
 url ={https://arxiv.org/pdf/2407.07858},
  year = {2024},
},
@article{turing1950computing,
  author    = {Alan M. Turing},
  title     = {Computing Machinery and Intelligence},
  journal  = {Mind},
  year      = {1950},
  volume   = {59},
  number   = {236},
  doi       = {10.1093/mind/LIX.236.433},
  url       = {https://academic.oup.com/mind/article/LIX/236/433/986238}
},
@misc{berry2024limits,
  author = {David M. Berry},
  title = {The Limits of Computation: Joseph Weizenbaum and the ELIZA Chatbot},
  institution = {University of Sussex},
  email = {d.m.berry@sussex.ac.uk},
  year = {2024},
  url = {https://ojs.weizenbaum-institut.de/index.php/wjds/article/view/106/3_3_2}
},
@misc{zemcik2024history,
  author = {Mgr. Tom{\'a}{\v{s}} ZEM{\v{C}}{\v{IK}}},
  title = {A Brief History of Chatbots},
  institution = {Department of Social Sciences, V{\v{S}}B - Technical University of Ostrava},
  address = {Czech Republic},
  year = {2024},
  url = {https://www.researchgate.net/publication/336734161_A_Brief_History_of_Chatbots}
},
@article{adamopoulou2020chatbots,
  author = {Eleni Adamopoulou and Lefteris Moussiade},
  title = {Chatbots: History, technology, and applications},
  journal = {ScienceDirect},
  year = {2020},
  url = {https://www.sciencedirect.com/science/article/pii/S2666827020300062}
},
@inproceedings{bellegarda2013siri,
  author = {Jerome R. Bellegarda},
  title = {Large--Scale Personal Assistant Technology Deployment: the Siri Experience},
  booktitle = {Interspeech 2013},
  year = {2013},
  url = {https://www.isca-archive.org/interspeech_2013/bellegarda13_interspeech.pdf}
},
@misc{apple2023siri,
  author = {Apple Inc.},
  title = {Siri - Apple},
  url = {https://www.apple.com/siri/},
  year = {2023}
},
@misc{su2023dragin,
  author  = {Weihang Su and Yichen Tang and Qingyao Ai and Zhijing Wu and Yiqun Liu},
  title = {DRAGIN: Dynamic Retrieval Augmented Generation based on the Information Needs of Large Language Models},
 ur; = {https://arxiv.org/pdf/2403.10081},
  institution  = {Department of Computer Science and Technology, Tsinghua University, and School of Computer Science and Technology, Beijing Institute of Technology},
  year = {2023}
},
@article{radford2018improving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={OpenAI},
  year={2018},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
},
@article{reddy2021customer,
  author = {Reddy, D.},
  title = {Increasing customer service efficiency through artificial intelligence chatbot},
  journal = {Revista de Gestão},
  volume  = {29},
  number  = {1},
  pages  ={19--33},
  year  = {2021},
  publisher = {Emerald Publishing Limited},
  doi = {10.1108/REGE-07-2021-0120},
  url = {https://www.emerald.com/insight/content/doi/10.1108/rege-07-2021-0120/full/pdf?title=increasing-customer-service-efficiency-through-artificial-intelligence-chatbot}
},
@inproceedings{karpukhin2020dense,
  title={Dense Passage Retrieval for Open-Domain Question Answering},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020}
},
@inproceedings{honovich2022true,
  title={TRUE: Re-evaluating Factual Consistency Evaluation},
  author={Honovich, Or and Scialom, Thomas and Ladhak, Faisal and Schick, Timo and Levy, Omer},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022}
},
@article{liu2023pre,
  title={Pre-train Prompting: What Works and Why},
  author={Liu, Pengfei and others},
  journal={Transactions of the Association for Computational Linguistics},
  year={2023}
},
@article{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
},
@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and others},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
},
@article{reynolds2021prompt,
  title={Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  journal={arXiv preprint arXiv:2102.07350},
  year={2021}
},
@misc{anthropic2024claude3,
  author       = {Anthropic},
  title        = {Introducing the next generation of Claude},
  year         = {2024},
  month        = {March},
  url          = {https://www.anthropic.com/news/claude-3-family},
},
@article{openai2023gpt4,
  author       = {OpenAI},
  title        = {GPT-4 Technical Report},
  journal      = {arXiv preprint arXiv:2303.08774},
  year         = 2023,
  url          = {https://arxiv.org/abs/2303.08774},
},
@misc{shuster2023ragas,
  author       = {Shuster, Kurt and Vyas, Dhruv and Mazaré, Pierre and Kiela, Douwe and Roller, Stephen},
  title        = {RAGAS: An Evaluation Framework for Retrieval Augmented Generation},
  year         = {2023},
  eprint       = {2309.14663},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/2309.14663}
},
@misc{devlin2018bert,
  author       = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title        = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year         = {2018},
  eprint       = {1810.04805},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url          = {https://arxiv.org/abs/1810.04805}
},
@article{brown2020language,
  title        = {Language Models are Few-Shot Learners},
  author       = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared D. Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal      = {Advances in Neural Information Processing Systems},
  volume       = {33},
  pages        = {1877--1901},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
},
@misc{myscale2024rag,
  author       = {{MyScale}},
  title        = {The Ultimate Guide to Evaluate RAG System Components},
  year         = {2024},
  url          = {https://myscale.com/blog/ultimate-guide-to-evaluate-rag-system/}
},
@misc{cloudflarehls,
  author       = {Cloudflare},
  title        = {What is HTTP Live Streaming?},
  year         = 2025,
  url          = {https://www.cloudflare.com/learning/video/what-is-http-live-streaming/}
},
@misc{mtebleaderboard,
  author       = {Muennighoff, Niklas and Tunstall, Lewis and Magne, Lo\"ic and Dettmers, Tim and Li, Teven and Berrios, Luis Fernando and Rimell, Laura and Williams, Adina and Schick, Timo and Ruder, Sebastian and Goyal, Naman and Dey, Daanish and Scao, Teven Le},
  title        = {Massive Text Embedding Benchmark (MTEB) Leaderboard},
  year         = {2023},
  url          = {https://huggingface.co/spaces/mteb/leaderboard},
  note         = {\url{https://huggingface.co/spaces/mteb/leaderboard}},
  howpublished = {Hugging Face Spaces}
},
@article{ragas2023,
  title={RAGAS: An Evaluation Framework for Retrieval-Augmented Generation},
  author={Madan, Aayush and Rai, Pulkit and Paranjape, Ashwini and Bhargava, Prahalika and Zareian, Alireza and Kale, Mihir and Dubey, Abhishek},
  journal={arXiv preprint arXiv:2309.15217},
  year={2023},
  url={https://arxiv.org/abs/2309.15217}
},
@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
},
@article{baevski2020wav2vec,
  title={wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
},
@article{jumper2021alphafold,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
},
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  note={https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
},
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Austin, Jacob and Dohan, David and others},
  journal={arXiv preprint arXiv:2206.14858},
  year={2022}
},
@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
},
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
},
@article{wei2022chain,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
},
@article{mialon2023augmented,
  title={Augmented Language Models: a Survey},
  author={Mialon, Guillaume and Touvron, Hugo and Levillain, Geoffrey and Lacroix, Timoth{\'e}e and Scialom, Thomas and Staerman, Guillaume and Sigtyar, Andrei and Lample, Guillaume and Jegou, Herv{\'e} and Gallinari, Patrick and others},
  journal={arXiv preprint arXiv:2302.07842},
  year={2023}
}